diff --git a/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp b/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp
index f6a08c33..de4aaa6a 100644
--- a/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp
+++ b/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp
@@ -1551,7 +1551,11 @@ void AqlQueue::ExecutePM4(uint32_t* cmd_data, size_t cmd_size_b, hsa_fence_scope
   memcpy(&queue_slot[1], &slot_data[1], slot_size_b - sizeof(uint32_t));
   if (core::Runtime::runtime_singleton_->flag().dev_mem_queue() && !agent_->is_xgmi_cpu_gpu()) {
     // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
     _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#endif
   }
   atomic::Store(&queue_slot[0], slot_data[0], std::memory_order_release);
 
diff --git a/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp b/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp
index 36d21fa1..c6d9c733 100644
--- a/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp
+++ b/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp
@@ -1274,7 +1274,11 @@ void BlitKernel::PopulateQueue(uint64_t index, uint64_t code_handle, void* args,
   std::atomic_thread_fence(std::memory_order_release);
   if (core::Runtime::runtime_singleton_->flag().dev_mem_queue() && !queue_->needsPcieOrdering()) {
     // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
     _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#endif
   }
   queue_buffer[index & queue_bitmask_].header = kDispatchPacketHeader;
 
diff --git a/runtime/hsa-runtime/core/runtime/intercept_queue.cpp b/runtime/hsa-runtime/core/runtime/intercept_queue.cpp
index a86dabb3..6377bc8a 100644
--- a/runtime/hsa-runtime/core/runtime/intercept_queue.cpp
+++ b/runtime/hsa-runtime/core/runtime/intercept_queue.cpp
@@ -258,7 +258,11 @@ uint64_t InterceptQueue::Submit(const AqlPacket* packets, uint64_t count) {
       ring[barrier & mask].barrier_and.completion_signal = Signal::Convert(async_doorbell_);
       if (Runtime::runtime_singleton_->flag().dev_mem_queue() && !needsPcieOrdering()) {
         // Ensure the packet body is written as header may get reordered when writing over PCIE
-        _mm_sfence();
+#if defined(_M_X64)
+    _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#endif
       }
       atomic::Store(&ring[barrier & mask].barrier_and.header, kBarrierHeader,
                     std::memory_order_release);
@@ -305,7 +309,11 @@ uint64_t InterceptQueue::Submit(const AqlPacket* packets, uint64_t count) {
       if (write_index != 0) {
         if (Runtime::runtime_singleton_->flag().dev_mem_queue() && !needsPcieOrdering()) {
           // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
           _mm_sfence();
+#elif defined(__loongarch_lp64)
+          asm("dbar 0");
+#endif
         }
         atomic::Store(&ring[write & mask].packet.header, packets[first_written_packet_index].packet.header,
                       std::memory_order_release);
@@ -374,7 +382,11 @@ void InterceptQueue::StoreRelaxed(hsa_signal_value_t value) {
     handler.first(&ring[i & mask], 1, i, handler.second, PacketWriter);
     if (Runtime::runtime_singleton_->flag().dev_mem_queue() && !needsPcieOrdering()) {
       // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
       _mm_sfence();
+#elif defined(__loongarch_lp64)
+      asm("dbar 0");
+#endif
     }
     // Invalidate consumed packet.
     atomic::Store(&ring[i & mask].packet.header, kInvalidHeader, std::memory_order_release);
diff --git a/runtime/hsa-runtime/core/util/locks.h b/runtime/hsa-runtime/core/util/locks.h
index 6c0de49a..052d04ac 100644
--- a/runtime/hsa-runtime/core/util/locks.h
+++ b/runtime/hsa-runtime/core/util/locks.h
@@ -72,7 +72,11 @@ class HybridMutex {
     while (!lock_.compare_exchange_strong(old, 1)) {
       cnt--;
       if (cnt > maxSpinIterPause) {
+#if defined(_M_X64)
         _mm_pause();
+#else
+        os::YieldThread();
+#endif
       } else if (cnt-- > maxSpinIterYield) {
         os::YieldThread();
       } else {
